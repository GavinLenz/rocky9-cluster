===== ./.gitignore =====
# Python build and cache
__pycache__/
*.py[cod]
*.pyo
*.pyd
*.egg
*.egg-info/
*.manifest
*.spec
.Python
.env
.venv/
venv/
pip-wheel-metadata/
wheels/
build/
dist/
.eggs/

# IDE and editor files
.vscode/
.idea/
*.swp
*.swo
*~
.DS_Store
Thumbs.db
.project
.pydevproject
.settings/
*.code-workspace

# Logs and runtime output
*.log
logs/
*.out
*.err
*.pid
nohup.out

# OS-level clutter
*.bak
*.tmp
*.temp
*.old

# Ansible artifacts
.ansible/
*.retry
*.facts
*.vault
*.tar.gz
*.zip
*.log
/inventory/inventory.json

# Cluster artifacts
/mnt/pxe/
/srv/pxe/
/srv/tftp/
/var/www/html/os/
/var/www/html/repos/
/cluster/shared/
/cluster/data/
/cluster/results/

# Local configs and secrets
.env
config/*.secret.yml
config/*.local.yml
config/private.yml
*.pem
*.key
*.crt
*.pub
*.id_rsa*
*.id_ed25519*

# Virtual environment / tooling
.venv/
.env/
.tox/
.pytest_cache/
.mypy_cache/
.ruff_cache/

# Temporary or analysis outputs
tmp/
scratch/
sandbox/
*.dat
*.csv

# Archive and image artifacts
*.tar
*.gz
*.bz2
*.xz
*.zip
*.img
*.iso
*.qcow2

# System-specific
.cache/
.local/
.bash_history
.history

# for me
_docs/
_*.md

===== ./.gitattributes =====
# Default settings
* text=auto eol=lf
*.sh text eol=lf
*.yml text eol=lf
*.yaml text eol=lf
*.j2 text eol=lf
*.py text eol=lf
*.cfg text eol=lf
*.toml text eol=lf
*.md text eol=lf
*.txt text eol=lf
*.conf text eol=lf
*.service text eol=lf

# Binary 
*.iso binary
*.img binary
*.bin binary

# Diff and merge preferences
*.py diff=python
*.yml diff=yaml
*.yaml diff=yaml
*.j2 diff=cpp
*.cfg diff=ini
*.md diff=markdown

# Mark scripts as executable when cloned
*.sh eol=lf text executable
bin/* eol=lf text executable
scripts/* eol=lf text executable

# License and text files normalized
LICENSE text eol=lf
README.md text eol=lf

# Force consistent text encoding
*.yml text working-tree-encoding=UTF-8
*.py text working-tree-encoding=UTF-8


===== ./.editorconfig =====
# This file is deliberately minimal and safe for an Ansible/Kickstart/cluster repo.

root = true

[*]
charset = utf-8
end_of_line = lf
insert_final_newline = true
trim_trailing_whitespace = true
indent_style = space
indent_size = 2

# Python files: industry standard 4-space indentation
[*.py]
indent_size = 4
max_line_length = 100

# YAML/Ansible/Jinja: standard 2-space indentation
[*.yaml]
indent_size = 2
[*.yml]
indent_size = 2
[*.j2]
indent_size = 2

# Shell scripts: 2-space indent, but no forced line length
[*.sh]
indent_size = 2

# Markdown: do not trim trailing whitespace (used for formatting)
[*.md]
trim_trailing_whitespace = false
indent_size = 2


===== ./requirements/dev.txt =====
ansible-core==2.15.8
ansible-lint
yamllint
black
ruff
isort
passlib
rich
PyYAML
python-dotenv
pre-commit
setuptools<81
mpi4py
netaddr


===== ./requirements/controller.txt =====
ansible-core==2.15.8
PyYAML>=6.0
rich>=13.0
passlib>=1.7.4
python-dotenv>=0.21
setuptools<81
mpi4py
netaddr


===== ./requirements/requirements.yml =====
---
collections:
  - name: ansible.posix
  - name: community.general
    version: ">=9.1.0,<10.0.0"
  - name: ansible.netcommon


===== ./ansible.cfg =====
[defaults]
roles_path = ./roles
inventory  = inventory/inventory.json
# Use system Python (default on Rocky 9) so Ansible can start before the
# controller role builds its own virtualenv.
interpreter_python = /usr/bin/python3
host_key_checking = False

# Caching (performance + reproducibility)
fact_caching = jsonfile
fact_caching_connection = .ansible_cache


===== ./.yamllint =====
---
extends: default

ignore: |
  .git
  .venv
  .cache
  .github/workflows

rules:
  indentation: disable
  braces:
    max-spaces-inside: 1
  line-length:
    max: 100
    level: warning
  comments:
    min-spaces-from-content: 1
  comments-indentation: false
  truthy:
    allowed-values: ['true', 'false', 'on', 'off', 'yes', 'no']
    check-keys: false
  document-start:
    level: warning
  new-line-at-end-of-file:
    level: warning
  octal-values:
    forbid-implicit-octal: true
    forbid-explicit-octal: true


===== ./.pre-commit-config.yaml =====
---
repos:
  - repo: local
    hooks:
      - id: black
        name: black
        entry: black
        language: system
        types: [python]
        args: ["--line-length", "100"]
      - id: ruff
        name: ruff
        entry: ruff
        language: system
        types: [python]
        args: ["check", "--fix"]
      - id: isort
        name: isort
        entry: isort
        language: system
        types: [python]
        args: ["--profile", "black", "--line-length", "100"]
      - id: yamllint
        name: yamllint
        entry: yamllint
        language: system
        types: [yaml]
      - id: ansible-lint
        name: ansible-lint
        entry: ansible-lint
        language: system
        types: [yaml]
        args: ["--config", ".ansible-lint.yml"]


===== ./.github/workflows/ci.yml =====
---
name: CI

on:
  push:
    branches:
      - main
      - 'dev/**'

jobs:
  validate:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python 3.9
        uses: actions/setup-python@v5
        with:
          python-version: "3.9"

      - name: Install project dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements/dev.txt

      - name: Install Ansible collections
        run: |
          ansible-galaxy collection install -r requirements/requirements.yml

      - name: Verify inventory generator
        run: |
          python inventory/generator.py --list > inventory/inventory.json
          test -s inventory/inventory.json

      - name: Lint Python
        run: |
          ruff check .
          black --check .

      - name: Lint YAML
        run: yamllint .

      - name: Lint Ansible
        run: ansible-lint


===== ./output.txt =====


===== ./config/pxe.yml =====
---
pxe:
  username: ansible

  vendor: rocky
  image: rocky9_minimal

  install:
    drive: nvme0n1
    disable_interfaces: []

  paths:
    iso_dir: /srv/pxe/iso
    iso_mount: /var/www/html/os
    http_root: /var/www/html
    tftp_root: /srv/tftp

  packages:
    base:
      - dnsmasq
      - tftp-server
      - httpd
      - ipxe-bootimgs
      - rsync
      - python3-libselinux
      - policycoreutils-python-utils
    efi: []

  ipxe:
    default_target: fresh
    timeout_ms: 5000
    menu:
      - key: i
        label: Fresh install
        target: fresh
      - key: l
        label: Boot from NVMe
        target: local

  role_packages:
    controller:
      - "@^minimal-environment"
    compute:
      - "@^minimal-environment"


===== ./config/nodes.yml =====
---
nodes:
  head:
    role: controller
    ip: 10.0.0.1
    mac: "84:a9:38:64:9e:bc"
    iface: enp5s0
    connection:
      ansible_connection: local
      ansible_host: localhost
      ansible_python_interpreter: /usr/bin/python3
    variables:
      ansible_become: true
      ansible_become_method: sudo
      ansible_become_user: root
      ansible_become_password: "your_sudo_password"

  node01:
    role: compute
    ip: 10.0.0.11
    mac:
      - "84:47:09:4d:5b:d6"
    connection:
      ansible_connection: ssh
      ansible_user: ansible
      ansible_python_interpreter: /usr/bin/python3

  node02:
    role: compute
    ip: 10.0.0.12
    mac:
      - "84:47:09:4d:57:37"
    connection:
      ansible_connection: ssh
      ansible_user: ansible
      ansible_python_interpreter: /usr/bin/python3

  node03:
    role: compute
    ip: 10.0.0.13
    mac:
      - "84:47:09:4d:51:76"
    connection:
      ansible_connection: ssh
      ansible_user: ansible
      ansible_python_interpreter: /usr/bin/python3


===== ./config/images.yml =====
---
images:
  rocky9_minimal:
    name: Rocky-9.6-x86_64-minimal.iso
    url: https://download.rockylinux.org/pub/rocky/9/isos/x86_64/Rocky-9.6-x86_64-minimal.iso
    kernel: os/images/pxeboot/vmlinuz
    initrd: os/images/pxeboot/initrd.img
    vendor: rocky
    description: Rocky Linux 9 minimal image.


===== ./config/roles.yml =====
---
roles:
  controller:
    stack:
      - controller_common
      - controller
      - pxe

  compute:
    stack:
      - compute_common


===== ./config/metadata.yml =====
---
metadata:
  name: micro-cluster
  description: HPC micro-cluster driven by Ansible automation.


===== ./config/net.yml =====
---
network:
  pxe_iface: enp5s0
  server_ip: 10.0.0.1
  subnet: 10.0.0.0/24
  netmask: 255.255.255.0
  gateway: null
  dns: 8.8.8.8
  dhcp_range:
    start: 10.0.0.10
    end: 10.0.0.15


===== ./.ansible-lint.yml =====
---
profile: min
skip_list:

- fqcn-builtins
- fqcn[action-core]
- no-changed-when
- experimental
- yaml[line-length]
- name[missing]
- var-naming[no-role-prefix]
- role-name
- args[module]

warn_list:

- yaml
- formatting
- empty-playbook
- risky-file-permissions

exclude_paths:

- meta/
- scripts/
- docs/


===== ./Makefile =====
.DEFAULT_GOAL := help
SHELL := /bin/bash

PYTHON ?= $(if $(VIRTUAL_ENV),$(VIRTUAL_ENV)/bin/python3,$(if $(wildcard .venv/bin/python3),./.venv/bin/python3,python3))
PIP := $(PYTHON) -m pip
ANSIBLE_CMD ?= ANSIBLE_NOCOWS=1 ansible-playbook
ANSIBLE_ARGS ?= -vv
ANSIBLE_PLAYBOOK := $(ANSIBLE_CMD) $(ANSIBLE_ARGS)
CACHE_DIR := .ansible_cache

RUFF := $(PYTHON) -m ruff
BLACK := $(PYTHON) -m black
YAMLLINT := $(PYTHON) -m yamllint
ANSIBLE_LINT := $(PYTHON) -m ansiblelint

INV_SCRIPT := inventory/generator.py
INV_JSON   := inventory/inventory.json
CONFIG_YAML := $(wildcard config/*.yml)

PLAYBOOK_DIR := playbooks
PLAYBOOK_CONTROLLER := $(PLAYBOOK_DIR)/controller.yml
PLAYBOOK_COMPUTE := $(PLAYBOOK_DIR)/compute.yml
PLAYBOOK_PXE := $(PLAYBOOK_DIR)/pxe.yml
PLAYBOOK_SCHEDULER := $(PLAYBOOK_DIR)/scheduler.yml
PLAYBOOK_VALIDATION := $(PLAYBOOK_DIR)/validation.yml
PLAYBOOK_SLURM := $(PLAYBOOK_DIR)/slurm.yml

.PHONY: help hashes inv inv-show inv-clean dev-venv venv-check lint format clean controller compute pxe scheduler validate site

help: ## Show available targets grouped by phase
	@echo "== Development =="
	@awk -F':.*## ' '/^[a-zA-Z0-9_.-]+:.*##/ && /venv|lint|format|check|test|clean/ { printf "  %-22s %s\n", $$1, $$2 }' $(MAKEFILE_LIST)
	@echo ""
	@echo "== Inventory & Introspection =="
	@awk -F':.*## ' '/^[a-zA-Z0-9_.-]+:.*##/ && /hashes|inv|inv-show|inv-clean/ { printf "  %-22s %s\n", $$1, $$2 }' $(MAKEFILE_LIST)
	@echo ""
	@echo "== Ansible =="
	@awk -F':.*## ' '/^[a-zA-Z0-9_.-]+:.*##/ && /controller|compute|pxe|scheduler|validate|slurm/ { printf "  %-22s %s\n", $$1, $$2 }' $(MAKEFILE_LIST)

hashes: ## Generate hashes for .env
	chmod +x ./scripts/generate_hashes.py
	@$(PYTHON) ./scripts/generate_hashes.py

inv: $(INV_JSON) ## Generate and cache dynamic inventory
	@echo "[OK] Inventory cached at $(INV_JSON)"

$(INV_JSON): $(INV_SCRIPT) $(CONFIG_YAML)
	@mkdir -p $(CACHE_DIR)
	@mkdir -p $(dir $(INV_JSON))
	@echo "[BUILD] Generating inventory..."
	@$(PYTHON) $(INV_SCRIPT) --list > $(INV_JSON)
	@echo "[DONE] Inventory generation complete."

inv-show: ## Print generated inventory JSON to stdout
	@$(PYTHON) $(INV_SCRIPT) --list | jq .

inv-clean: ## Remove cached inventory and Ansible fact cache
	@rm -rf $(INV_JSON) $(CACHE_DIR)
	@echo "[CLEAN] Inventory cache removed."

venv: ## Create and initialize Python virtual environment
	@chmod +x scripts/dev_venv.sh
	@sudo scripts/dev_venv.sh
	@echo "[OK] Virtual environment ready."

venv-check: ## Verify Python 3.9+ and Ansible availability
	@command -v $(PYTHON) >/dev/null 2>&1 || { echo "Python 3 not found"; exit 1; }
	@$(PYTHON) -m ansible --version >/dev/null 2>&1 || { echo "Ansible not available"; exit 1; }
	@echo "[OK] Python and Ansible detected."

lint: ## Run all linters (Python, YAML, Ansible)
	@$(BLACK) --check .
	@$(RUFF) check .
	@$(YAMLLINT) .
	@$(ANSIBLE_LINT)
	@echo "[OK] Lint checks complete."

format: ## Auto-format Python and YAML
	@$(BLACK) .
	@$(RUFF) check --fix .
	@echo "[OK] Formatting complete."

clean: ## Remove caches, venv, and temporary files
	chmod +x ./scripts/clean.py
	./scripts/clean.py

# Playbook runners (auto-generate inventory first)
controller: inv ## Run controller playbook (PXE + controller + scheduler)
	@$(ANSIBLE_PLAYBOOK) -i $(INV_JSON) $(PLAYBOOK_CONTROLLER)

compute: inv ## Run compute playbook (common + scheduler)
	@$(ANSIBLE_PLAYBOOK) -i $(INV_JSON) $(PLAYBOOK_COMPUTE)

pxe: inv ## Run PXE-only playbook
	@$(ANSIBLE_PLAYBOOK) -i $(INV_JSON) $(PLAYBOOK_PXE)

scheduler: inv ## Run scheduler-only playbook
	@$(ANSIBLE_PLAYBOOK) -i $(INV_JSON) $(PLAYBOOK_SCHEDULER)

validate: inv ## Run validation playbook on controller
	@$(ANSIBLE_PLAYBOOK) -i $(INV_JSON) $(PLAYBOOK_VALIDATION)

slurm: inv ## Run slurm playbook on controller
	@$(ANSIBLE_PLAYBOOK) -i $(INV_JSON) $(PLAYBOOK_SLURM)


===== ./LICENSE =====
MIT License

Copyright (c) 2025 Gavin

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

===== ./playbooks/validation.yml =====
---
- name: Run validation tasks on controller
  hosts: controller
  become: true
  gather_facts: false
  roles:
    - role: validation
      tags:
        - validation


===== ./playbooks/pxe.yml =====
---
- name: Run PXE stack
  hosts: controller
  become: yes
  roles:
    - role: pxe
      tags:
        - pxe


===== ./playbooks/compute.yml =====
---
- name: Configure compute nodes
  hosts: compute
  become: true
  roles:
    - role: compute_common


===== ./playbooks/controller.yml =====
---
- name: Configure controller node
  hosts: controller
  become: yes
  roles:
    - role: controller_common
    - role: controller
    - role: pxe


===== ./inventory/__init__.py =====


===== ./inventory/inventory.example.json =====


===== ./inventory/inventory.json =====
{
  "all": {
    "children": {
      "controller": {
        "hosts": {
          "head": {
            "ansible_host": "localhost",
            "ansible_connection": "local",
            "ansible_python_interpreter": "/usr/bin/python3",
            "cluster_role": "controller",
            "pxe_iface": "enp5s0",
            "pxe_server_ip": "10.0.0.1",
            "pxe_install_drive": "nvme0n1",
            "pxe_default_target": "fresh",
            "pxe_ipxe_menu_items": [
              {
                "key": "i",
                "label": "Fresh install",
                "target": "fresh"
              },
              {
                "key": "l",
                "label": "Boot from NVMe",
                "target": "local"
              }
            ],
            "ansible_become": true,
            "ansible_become_method": "sudo",
            "ansible_become_user": "root",
            "ansible_become_password": "your_sudo_password"
          }
        }
      },
      "compute": {
        "hosts": {
          "node01": {
            "ansible_host": "10.0.0.11",
            "ansible_connection": "ssh",
            "ansible_user": "ansible",
            "ansible_python_interpreter": "/usr/bin/python3",
            "cluster_role": "compute",
            "nic_mac": [
              "84:47:09:4d:5b:d6"
            ]
          },
          "node02": {
            "ansible_host": "10.0.0.12",
            "ansible_connection": "ssh",
            "ansible_user": "ansible",
            "ansible_python_interpreter": "/usr/bin/python3",
            "cluster_role": "compute",
            "nic_mac": [
              "84:47:09:4d:57:37"
            ]
          },
          "node03": {
            "ansible_host": "10.0.0.13",
            "ansible_connection": "ssh",
            "ansible_user": "ansible",
            "ansible_python_interpreter": "/usr/bin/python3",
            "cluster_role": "compute",
            "nic_mac": [
              "84:47:09:4d:51:76"
            ]
          }
        }
      }
    },
    "vars": {
      "cluster_name": "cluster",
      "cluster_description": "",
      "cluster_repo_root": "/root/work/rocky9-cluster",
      "cluster_config_root": "/root/work/rocky9-cluster/config",
      "cluster_roles": {
        "controller": {
          "stack": [
            "controller_common",
            "controller",
            "pxe"
          ]
        },
        "compute": {
          "stack": [
            "compute_common"
          ]
        }
      },
      "network_config": {
        "pxe_iface": "enp5s0",
        "server_ip": "10.0.0.1",
        "subnet": "10.0.0.0/24",
        "netmask": "255.255.255.0",
        "gateway": null,
        "dns": "8.8.8.8",
        "dhcp_range": {
          "start": "10.0.0.10",
          "end": "10.0.0.15"
        }
      },
      "pxe_config": {
        "username": "ansible",
        "vendor": "rocky",
        "image": "rocky9_minimal",
        "install": {
          "drive": "nvme0n1",
          "disable_interfaces": []
        },
        "paths": {
          "iso_dir": "/srv/pxe/iso",
          "iso_mount": "/var/www/html/os",
          "http_root": "/var/www/html",
          "tftp_root": "/srv/tftp"
        },
        "packages": {
          "base": [
            "dnsmasq",
            "tftp-server",
            "httpd",
            "ipxe-bootimgs",
            "rsync",
            "python3-libselinux",
            "policycoreutils-python-utils"
          ],
          "efi": []
        },
        "ipxe": {
          "default_target": "fresh",
          "timeout_ms": 5000,
          "menu": [
            {
              "key": "i",
              "label": "Fresh install",
              "target": "fresh"
            },
            {
              "key": "l",
              "label": "Boot from NVMe",
              "target": "local"
            }
          ]
        },
        "role_packages": {
          "controller": [
            "@^minimal-environment"
          ],
          "compute": [
            "@^minimal-environment"
          ]
        }
      },
      "images": {
        "rocky9_minimal": {
          "name": "Rocky-9.6-x86_64-minimal.iso",
          "url": "https://download.rockylinux.org/pub/rocky/9/isos/x86_64/Rocky-9.6-x86_64-minimal.iso",
          "kernel": "os/images/pxeboot/vmlinuz",
          "initrd": "os/images/pxeboot/initrd.img",
          "vendor": "rocky",
          "description": "Rocky Linux 9 minimal image."
        }
      },
      "pxe_runtime_credentials": {
        "username": "ansible",
        "root_password_hash": null,
        "local_user_password_hash": null
      }
    }
  }
}


===== ./inventory/generator.py =====
#!/usr/bin/env python3

from __future__ import annotations

import argparse
import json
from pathlib import Path
from typing import Any, Iterable

import yaml
from dotenv import dotenv_values

ROOT = Path(__file__).resolve().parents[1]
CONFIG_DIR = ROOT / "config"
# Load secrets from .env (if present)
ENV = dotenv_values(ROOT / ".env")


CONFIG_FILES = {
    "images": CONFIG_DIR / "images.yml",
    "metadata": CONFIG_DIR / "metadata.yml",
    "net": CONFIG_DIR / "net.yml",
    "nodes": CONFIG_DIR / "nodes.yml",
    "pxe": CONFIG_DIR / "pxe.yml",
    "roles": CONFIG_DIR / "roles.yml",
}


def load_yaml(path: Path) -> dict[str, Any]:
    if not path.exists():
        return {}
    with path.open("r", encoding="utf-8") as f:
        data = yaml.safe_load(f)
        return data if isinstance(data, dict) else {}


def build_inventory() -> tuple[dict[str, Any], dict[str, Any]]:
    """Build minimal static inventory from config/.

    Returns the inventory structure and a flattened hostvars mapping so we can
    still service `--host` lookups when invoked as a dynamic inventory script.
    """
    cfg = {name: load_yaml(path) for name, path in CONFIG_FILES.items()}

    # Names for inventory
    nodes = cfg.get("nodes", {}).get("nodes", {})
    roles = cfg.get("roles", {}).get("roles", {})
    net = cfg.get("net", {}).get("network", {})
    pxe = cfg.get("pxe", {}).get("pxe", {})

    # Load secret credentials from .env
    root_hash = ENV.get("PXE_ROOT_PASSWORD_HASH")
    local_hash = ENV.get("PXE_LOCAL_USER_PASSWORD_HASH")

    # Assignment
    controller_nodes = {n: c for n, c in nodes.items() if c.get("role") == "controller"}
    compute_nodes = {n: c for n, c in nodes.items() if c.get("role") == "compute"}

    if not controller_nodes and nodes:
        first = next(iter(nodes))
        controller_nodes[first] = nodes[first]
        compute_nodes.pop(first, None)

    hostvars: dict[str, Any] = {}

    # Controller vars
    for name, node in controller_nodes.items():
        conn = node.get("connection", {})
        hostvars[name] = {
            "ansible_host": conn.get("ansible_host", node.get("ip", name)),
            "ansible_connection": conn.get("ansible_connection", "local"),
            "ansible_python_interpreter": conn.get(
                "ansible_python_interpreter", "/usr/bin/python3"
            ),
            "cluster_role": "controller",
            "pxe_iface": net.get("pxe_iface", ""),
            "pxe_server_ip": net.get("server_ip", ""),
            "pxe_install_drive": pxe.get("install", {}).get("drive", ""),
            "pxe_default_target": pxe.get("ipxe", {}).get("default_target", ""),
            "pxe_ipxe_menu_items": pxe.get("ipxe", {}).get("menu", []),
        }
        # Allow arbitrary host-level vars from config/nodes.yml
        hostvars[name].update(node.get("variables", {}))

    # Compute vars
    for name, node in compute_nodes.items():
        conn = node.get("connection", {})
        hostvars[name] = {
            "ansible_host": conn.get("ansible_host", node.get("ip", name)),
            "ansible_connection": conn.get("ansible_connection", "ssh"),
            "ansible_user": conn.get("ansible_user", "ansible"),
            "ansible_python_interpreter": conn.get(
                "ansible_python_interpreter", "/usr/bin/python3"
            ),
            "cluster_role": "compute",
            "nic_mac": node.get("mac", []),
        }
        hostvars[name].update(node.get("variables", {}))

    # Global vars
    all_vars = {
        "cluster_name": cfg.get("metadata", {}).get("name", "cluster"),
        "cluster_description": cfg.get("metadata", {}).get("description", ""),
        "cluster_repo_root": str(ROOT),
        "cluster_config_root": str(CONFIG_DIR),
        "cluster_roles": roles,
        "network_config": net,
        "pxe_config": pxe,
        "images": cfg.get("images", {}).get("images", {}),
        "pxe_runtime_credentials": {
            "username": pxe.get("username", "ansible"),
            # Injected from .env
            "root_password_hash": root_hash,
            "local_user_password_hash": local_hash or root_hash,
        },
    }

    # Embed host-specific vars directly under each host entry so Ansible's YAML
    # inventory plugin sees them without relying on `_meta.hostvars`.
    controller_hosts = {name: hostvars[name] for name in controller_nodes}
    compute_hosts = {name: hostvars[name] for name in compute_nodes}

    inventory = {
        "all": {
            "children": {
                "controller": {"hosts": controller_hosts},
                "compute": {"hosts": compute_hosts},
            },
            "vars": all_vars,
        }
    }

    return inventory, hostvars


def parse_args(argv: Iterable[str] | None = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Cluster dynamic inventory")
    parser.add_argument("--host", type=str)
    parser.add_argument("--list", action="store_true")
    return parser.parse_args(argv)


def main(argv: Iterable[str] | None = None) -> int:
    args = parse_args(argv)
    inventory, hostvars = build_inventory()

    output_path = Path(__file__).resolve().parent / "inventory.json"

    if args.host:
        hv = hostvars.get(args.host, {})
        print(json.dumps(hv, indent=2))
    else:
        print(json.dumps(inventory, indent=2))

    with output_path.open("w", encoding="utf-8") as f:
        json.dump(inventory, f, indent=2, ensure_ascii=False)
        f.write("\n")

    return 0


if __name__ == "__main__":
    raise SystemExit(main())


===== ./scripts/dev_venv.sh =====
#!/bin/bash
# run on dev machine (not a cluster node)

set -euo pipefail

log() { printf "[venv] %s\n" "$*" >&2; }

python_is_39() {
    local bin="$1"

    if ! command -v "$bin" >/dev/null 2>&1; then
        return 1
    fi

    # Extract major/minor reliably
    local major minor
    major="$("$bin" -c 'import sys; print(sys.version_info[0])')"
    minor="$("$bin" -c 'import sys; print(sys.version_info[1])')"

    (( major == 3 && minor == 9 ))
}

ensure_python39_installed() {
    if python_is_39 python3; then
        log "Python 3.9 already installed."
        return 0
    fi

    log "Installing Python 3.9..."
    dnf -y install python3 python3-devel openmpi openmpi-devel|| {
        log "Failed to install python3"
        exit 1
    }

    if ! python_is_39 python3; then
        log "Python 3 installation failed."
        exit 1
    fi

    log "Python 3 installation confirmed."
}

create_venv() {
    local repo_root="$1"
    local venv_path="${repo_root}/.venv"

    local owner user_home
    owner="$(stat -c '%U' "$repo_root")"

    log "Creating venv at: ${venv_path}"
    log "Creating as owner: ${owner}"

    runuser -u "${owner}" -- bash -lc "
        python3 -m venv '${venv_path}'
        '${venv_path}/bin/python' -m pip install --upgrade pip setuptools wheel
        '${venv_path}/bin/pip' install -r '${repo_root}/requirements/dev.txt'
        '${venv_path}/bin/ansible-galaxy' collection install ansible.posix community.general ansible.netcommon
        '${venv_path}/bin/pre-commit' install
        "
    log "venv created and packages installed."
}

# ENTRYPOINT
if [[ "$(id -u)" -ne 0 ]]; then
    log "Run with sudo/root."
    exit 1
fi

ensure_python39_installed

# Resolve repo root: script resides under repo/scripts/, so go up one level.
REPO_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
log "Repo root resolved as: ${REPO_ROOT}"

create_venv "${REPO_ROOT}"

log "===================================================================="
log "Virtualenv ready:"
log "  source ${REPO_ROOT}/.venv/bin/activate"
log "===================================================================="


===== ./scripts/clean.py =====
#!/usr/bin/env python3
"""
Removes cache directories and Python bytecode without
risking accidental deletion outside the repository root.
"""

from __future__ import annotations

import argparse
import sys
from pathlib import Path
from typing import Iterable

# Default patterns and directories to remove
DEFAULT_PATTERNS = (
    "__pycache__",
    "*.pyc",
    "*.pyo",
    ".pytest_cache",
    ".mypy_cache",
    ".ruff_cache",
)

DEFAULT_DIRS = (
    ".ansible",
    ".pytest_cache",
    ".mypy_cache",
    ".ruff_cache",
)

# Directories that will never be removed
PROTECTED = {
    "/",
    "/root",
    "/home",
    "/usr",
    "/etc",
    "/var",
    "/opt",
}


def is_protected(path: Path) -> bool:
    """Reject unsafe deletions and strange roots."""
    try:
        rp = str(path.resolve())
    except Exception:
        return True

    if rp in PROTECTED:
        return True

    # Reject oddities like removing the repository root itself
    if path == path.root:
        return True

    return False


def log(msg: str) -> None:
    print(f"[cleanup] {msg}")


def warn_skip(path: Path, exc: Exception) -> None:
    sys.stderr.write(f"[cleanup] warning: could not remove {path}: {exc}\n")


def remove_path(path: Path, *, dry: bool) -> None:
    """Recursively remove file or directory."""
    if is_protected(path):
        warn_skip(path, PermissionError("protected path"))
        return

    if dry:
        log(f"would remove {path}")
        return

    if path.is_dir():
        try:
            for item in path.iterdir():
                remove_path(item, dry=dry)
            path.rmdir()
            log(f"removed dir {path}")
        except Exception as exc:
            warn_skip(path, exc)
    else:
        try:
            path.unlink(missing_ok=True)
            log(f"removed file {path}")
        except Exception as exc:
            warn_skip(path, exc)


def cleanup(root: Path, patterns: Iterable[str], dirs: Iterable[str], *, dry: bool) -> None:
    for d in dirs:
        for p in root.rglob(d):
            if p.is_dir():
                remove_path(p, dry=dry)

    for pattern in patterns:
        for path in root.rglob(pattern):
            remove_path(path, dry=dry)


def parse() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Repository cleanup utility")
    parser.add_argument("--root", type=Path, default=Path.cwd())
    parser.add_argument("--pattern", action="append", dest="patterns")
    parser.add_argument("--dir", action="append", dest="dirs")
    parser.add_argument("--dry-run", action="store_true", dest="dry")
    return parser.parse_args()


def main(argv=None) -> int:
    args = parse()

    root = args.root.resolve()

    if is_protected(root):
        sys.stderr.write(f"[ERROR] Refusing to clean protected path: {root}\n")
        return 1

    patterns = tuple(args.patterns) if args.patterns else DEFAULT_PATTERNS
    dirs = tuple(args.dirs) if args.dirs else DEFAULT_DIRS

    log(f"starting cleanup under {root}")
    if args.dry:
        log("dry-run enabled (no changes will be made)")

    cleanup(root, patterns, dirs, dry=args.dry)
    log("cleanup complete")

    return 0


if __name__ == "__main__":
    raise SystemExit(main())


===== ./scripts/generate_hashes.py =====
#!/usr/bin/env python3

import getpass
import sys

from passlib.hash import sha512_crypt


def main():
    print("Enter password to hash (input hidden):")
    pw = getpass.getpass()

    if not pw:
        print("Error: empty password not allowed")
        sys.exit(1)

    hash_val = sha512_crypt.hash(pw)

    print("\nGenerated SHA-512 password hash:")
    print(hash_val)
    print("\nCopy and paste these lines into your .env file:")
    print(f'PXE_ROOT_PASSWORD_HASH="{hash_val}"')
    print(f'PXE_LOCAL_USER_PASSWORD_HASH="{hash_val}"')


if __name__ == "__main__":
    main()


===== ./roles/controller_common/defaults/main.yml =====
---
common_groups:
  - name: cluster
    system: true

common_packages:
  - vim
  - git
  - curl
  - net-tools
  - python3
  - python3-devel
  - python3-pip
  - rsync
  - tmux
  - policycoreutils-python-utils
  - python3-netaddr

common_users:
  - name: ansible
    groups: [wheel]
    shell: /bin/bash
    authorized_key: "{{ lookup('file', '~/.ssh/id_ed25519.pub', errors='ignore') }}"

common_sysctl:
  vm.swappiness: 10
  net.ipv4.ip_forward: 1

common_limits:
  - domain: "*"
    type: "soft"
    item: "nofile"
    value: 65535
  - domain: "*"
    type: "hard"
    item: "nofile"
    value: 65535

server_ip: "{{ network_config.server_ip }}"
controller_core_repo_default_distribution: rocky
controller_core_repo_map:
  rocky:
    - baseos
    - appstream
  almalinux:
    - baseos
    - appstream
  redhat:
    - rhel-9-for-x86_64-baseos-rpms
    - rhel-9-for-x86_64-appstream-rpms
  "red hat enterprise linux":
    - rhel-9-for-x86_64-baseos-rpms
    - rhel-9-for-x86_64-appstream-rpms
  redhatenterpriseserver:
    - rhel-9-for-x86_64-baseos-rpms
    - rhel-9-for-x86_64-appstream-rpms
  rhel:
    - rhel-9-for-x86_64-baseos-rpms
    - rhel-9-for-x86_64-appstream-rpms
controller_core_repo_ids: []
slurm_repo_gpgcheck: "{{ slurm_mirror.gpgcheck | default(0) }}"


===== ./roles/controller_common/tasks/main.yml =====
---
# 0. Create required groups FIRST
- name: Ensure cluster groups exist
  ansible.builtin.group:
    name: "{{ item.name }}"
    state: present
    system: "{{ item.system | default(true) }}"
  loop: "{{ common_groups }}"

# 1. Create cluster users
- name: Create cluster users
  ansible.builtin.user:
    name: "{{ item.name }}"
    groups: "{{ item.groups | default([]) }}"
    shell: "{{ item.shell | default('/bin/bash') }}"
    state: present
  loop: "{{ common_users }}"

# 2. Allow passwordless sudo for automation user
- name: Allow passwordless sudo for automation user
  ansible.builtin.copy:
    dest: /etc/sudoers.d/99-ansible-nopasswd
    content: "ansible ALL=(ALL) NOPASSWD:ALL\n"
    owner: root
    group: root
    mode: '0440'

# 3. Install authorized SSH keys
- name: Install authorized SSH keys
  ansible.posix.authorized_key:
    user: "{{ item.name }}"
    key: "{{ item.authorized_key }}"
    state: present
  loop: "{{ common_users | selectattr('authorized_key','defined') }}"
  when: item.authorized_key | length > 0

# 4. Install baseline packages
- name: Ensure baseline packages installed
  ansible.builtin.package:
    name: "{{ common_packages }}"
    state: present

# 5. Apply sysctl settings
- name: Apply sysctl defaults
  ansible.posix.sysctl:
    name: "{{ item.key }}"
    value: "{{ item.value }}"
    state: present
    reload: yes
  loop: "{{ common_sysctl | dict2items }}"

# 6. Apply PAM limits
- name: Apply ulimit settings
  community.general.pam_limits:
    domain: "{{ item.domain }}"
    limit_type: "{{ item.type }}"
    limit_item: "{{ item.item }}"
    value: "{{ item.value }}"
  loop: "{{ common_limits }}"


===== ./roles/compute_common/defaults/main.yml =====
---
common_groups:
  - name: cluster
    system: true

common_packages:
  - vim
  - git
  - curl
  - net-tools
  - python3
  - python3-devel
  - python3-pip
  - rsync
  - tmux

common_users:
  - name: ansible
    groups: [wheel]
    shell: /bin/bash
    authorized_key: "{{ lookup('file', '~/.ssh/id_ed25519.pub', errors='ignore') }}"

common_sysctl:
  vm.swappiness: 10
  net.ipv4.ip_forward: 1

common_limits:
  - domain: "*"
    type: "soft"
    item: "nofile"
    value: 65535
  - domain: "*"
    type: "hard"
    item: "nofile"
    value: 65535

server_ip: "{{ network_config.server_ip }}"
slurm_repo_gpgcheck: "{{ slurm_mirror.gpgcheck | default(0) }}"


===== ./roles/compute_common/tasks/main.yml =====
---
# 0. Create required groups FIRST
- name: Ensure cluster groups exist
  ansible.builtin.group:
    name: "{{ item.name }}"
    state: present
    system: "{{ item.system | default(true) }}"
  loop: "{{ common_groups }}"
  tags:
    - common
    - users

# 1. Create cluster users
- name: Create cluster users
  ansible.builtin.user:
    name: "{{ item.name }}"
    groups: "{{ item.groups | default([]) }}"
    shell: "{{ item.shell | default('/bin/bash') }}"
    state: present
  loop: "{{ common_users }}"
  tags:
    - common
    - users

# 2. Allow passwordless sudo for automation user
- name: Allow passwordless sudo for automation user
  ansible.builtin.copy:
    dest: /etc/sudoers.d/99-ansible-nopasswd
    content: "ansible ALL=(ALL) NOPASSWD:ALL\n"
    owner: root
    group: root
    mode: '0440'
  tags:
    - common
    - sudo

# 3. Install authorized SSH keys (NOW VALID)
- name: Install authorized SSH keys
  ansible.posix.authorized_key:
    user: "{{ item.name }}"
    key: "{{ item.authorized_key }}"
    state: present
  loop: "{{ common_users | selectattr('authorized_key','defined') }}"
  when: item.authorized_key | length > 0
  tags:
    - common
    - users

# 4. Deploy internal repo file
- name: Deploy internal cluster repo configuration
  ansible.builtin.template:
    src: local.repo.j2
    dest: /etc/yum.repos.d/cluster.repo
    owner: root
    group: root
    mode: '0644'
  vars:
    controller_ip: "{{ hostvars[groups['controller'][0]].ip }}"
  when: "'controller' not in group_names"
  tags:
    - repo

# 5. Rebuild yum metadata
- name: Clean DNF metadata
  ansible.builtin.command: dnf clean all
  changed_when: false
  when: "'controller' not in group_names"
  tags:
    - repo

- name: Rebuild DNF metadata cache
  ansible.builtin.command: dnf makecache
  changed_when: false
  when: "'controller' not in group_names"
  tags:
    - repo

# 6. Install baseline packages
- name: Ensure baseline packages installed
  ansible.builtin.package:
    name: "{{ common_packages }}"
    state: present
  tags:
    - common
    - bootstrap

# 7. Apply sysctl settings
- name: Apply sysctl defaults
  ansible.posix.sysctl:
    name: "{{ item.key }}"
    value: "{{ item.value }}"
    state: present
    reload: yes
  loop: "{{ common_sysctl | dict2items }}"
  tags:
    - common
    - sysctl

# 8. Apply PAM limits
- name: Apply ulimit settings
  community.general.pam_limits:
    domain: "{{ item.domain }}"
    limit_type: "{{ item.type }}"
    limit_item: "{{ item.item }}"
    value: "{{ item.value }}"
  loop: "{{ common_limits }}"
  tags:
    - common
    - limits


===== ./roles/validation/templates/pavilion.yaml.j2 =====
---
# Pavilion global configuration, used when we run:
#   pav --config "{{ validation_pavilion.config_dir }}" ...

config_dirs:
  - "{{ validation_pavilion.config_dir }}"

working_dir: "{{ validation_pavilion.work_dir }}"

# Kind of verbose; useful for debugging
log_level: "INFO"

# Can add additional plugin config here...


===== ./roles/validation/defaults/main.yml =====
---
validation_pavilion:
  enabled: true

  # Where Pavilion2 source will be installed (git clone).
  install_dir: /opt/pavilion2

  # Config dir that Pavilion will use. We’ll copy your repo configs here.
  config_dir: /opt/pavilion2/config

  # Working dir for Pavilion runs (results, logs).
  work_dir: /opt/pavilion2/work

  # Pavilion git repo (LANL public).
  repo_url: "https://github.com/lanl/Pavilion2.git"
  repo_version: "main"

  # Which Pavilion suite (file prefix) to execute by default.
  default_suite: "controller_smoke"


===== ./roles/validation/pavilion/hosts/cluster.yml =====
---
# Host config applied when we run with: pav -H cluster ...
# This is a full test config overlay per Pavilion docs. :contentReference[oaicite:3]{index=3}

scheduler: raw

schedule:
  nodes: 1
  tasks_per_node: 1


===== ./roles/validation/pavilion/suites/controller_smoke.yml =====
---
controller_dnsmasq:
  summary: "dnsmasq service is active"
  scheduler: raw
  run:
    cmds:
      - |
        set -eu
        state=$(systemctl is-active dnsmasq)
        echo "STATE=${state}"
  result_parse:
    regex:
      regex: '^STATE=(?P<state>\w+)$'
      matches: one
  result_evaluate:
    ok:
      formula: "result.regex.state == 'active'"
      result: "PASS"
      fail_result: "FAIL"

controller_httpd:
  summary: "httpd service is active"
  scheduler: raw
  run:
    cmds:
      - |
        set -eu
        state=$(systemctl is-active httpd)
        echo "STATE=${state}"
  result_parse:
    regex:
      regex: '^STATE=(?P<state>\w+)$'
      matches: one
  result_evaluate:
    ok:
      formula: "result.regex.state == 'active'"
      result: "PASS"
      fail_result: "FAIL"

controller_firewall:
  summary: "Firewall exposes PXE services (http, tftp, dhcp)"
  scheduler: raw
  run:
    cmds:
      - |
        set -eu
        services=$(firewall-cmd --list-services | tr -s '[:space:]' ' ')
        echo "SERVICES=${services}"
  result_parse:
    regex:
      regex: '^SERVICES=(?P<services>.+)$'
      matches: one
  result_evaluate:
    ok:
      formula: "all(svc in result.regex.services.split() for svc in ['http','tftp','dhcp'])"
      result: "PASS"
      fail_result: "FAIL"

controller_repo:
  summary: "Mirror repository tree exists"
  scheduler: raw
  run:
    cmds:
      - |
        set -eu
        count=$(ls -1 /var/www/html/repos | wc -l)
        echo "REPO_COUNT=${count}"
  result_parse:
    regex:
      regex: '^REPO_COUNT=(?P<count>\d+)$'
      matches: one
  result_evaluate:
    ok:
      formula: "int(result.regex.count) >= 1"
      result: "PASS"
      fail_result: "FAIL"


===== ./roles/validation/pavilion/suites/basic_smoke.yml =====
---
network_gateway_ping:
  summary: "Controller can reach the configured gateway"
  scheduler: raw
  run:
    cmds:
      - |
        set -eu
        ping -c 4 {{ network_config.gateway | default(network_config.server_ip, true) }}
  result_parse:
    regex:
      regex: '(\d+) packets transmitted, (\d+) received'
      matches: one
  result_evaluate:
    ok:
      formula: "int(result.regex._2) >= 1"
      result: "PASS"
      fail_result: "FAIL"

pxe_tree_check:
  summary: "PXE ISO tree is mounted under /var/www/html/os"
  scheduler: raw
  run:
    cmds:
      - |
        set -eu
        if [[ -f /var/www/html/os/BaseOS/repodata/repomd.xml ]]; then
          echo "PXE_TREE=present"
        else
          echo "PXE_TREE=missing"
          exit 1
        fi
  result_parse:
    regex:
      regex: '^PXE_TREE=(?P<state>\w+)$'
      matches: one
  result_evaluate:
    ok:
      formula: "result.regex.state == 'present'"
      result: "PASS"
      fail_result: "FAIL"

pxe_http_probe:
  summary: "PXE content is served over HTTP"
  scheduler: raw
  run:
    cmds:
      - |
        set -eu
        curl -sfI "http://{{ network_config.server_ip }}/os/" >/dev/null
        echo "PXE_HTTP=ok"
  result_parse:
    regex:
      regex: '^PXE_HTTP=(?P<state>\w+)$'
      matches: one
  result_evaluate:
    ok:
      formula: "result.regex.state == 'ok'"
      result: "PASS"
      fail_result: "FAIL"


===== ./roles/validation/LICENSE =====
This role uses Pavilion2 (LA-UR-19-27077, BSD-3-Clause).

Pavilion2 is © Triad National Security, LLC. All rights reserved.
See https://github.com/lanl/Pavilion2 for full license text.

===== ./roles/validation/tasks/main.yml =====
---
- name: Validation
  when: inventory_hostname in groups['controller']
  block:
    - name: Include Pavilion install tasks
      ansible.builtin.import_tasks: pavilion_install.yml
      tags:
        - validation
        - validation_pavilion
        - validation_install

    - name: Include Pavilion config tasks
      ansible.builtin.import_tasks: pavilion_config.yml
      tags:
        - validation
        - validation_pavilion
        - validation_config

    - name: Optionally run Pavilion smoke tests
      ansible.builtin.import_tasks: pavilion_run.yml
      when: validation_pavilion.enabled | bool
      tags:
        - validation
        - validation_pavilion
        - validation_run


===== ./roles/validation/tasks/pavilion_run.yml =====
---
- name: Run Pavilion validation suite
  ansible.builtin.command:
    cmd: >
      {{ validation_pavilion.install_dir }}/venv/bin/pav
      --config "{{ validation_pavilion.config_dir }}"
      -H cluster
      run {{ validation_pavilion.default_suite }}.*
  args:
    chdir: "{{ validation_pavilion.install_dir }}"
  register: pavilion_run_result
  changed_when: false

- name: Show Pavilion run summary
  ansible.builtin.debug:
    var: pavilion_run_result.stdout


===== ./roles/validation/tasks/pavilion_install.yml =====
---
- name: Ensure Pavilion OS dependencies are present
  ansible.builtin.package:
    name: # should already be present
      - git
      - python3
      - python3-pip
      - python3-virtualenv
    state: present

- name: Create Pavilion install parent dir
  ansible.builtin.file:
    path: "{{ validation_pavilion.install_dir }}"
    state: directory
    mode: "0755"

- name: Clone Pavilion2 repository
  ansible.builtin.git:
    repo: "{{ validation_pavilion.repo_url }}"
    dest: "{{ validation_pavilion.install_dir }}"
    version: "{{ validation_pavilion.repo_version }}"
    update: yes

- name: Ensure Pavilion working directory exists
  ansible.builtin.file:
    path: "{{ validation_pavilion.work_dir }}"
    state: directory
    mode: "0775"

- name: Ensure Pavilion virtualenv exists
  ansible.builtin.command: python3 -m venv venv
  args:
    chdir: "{{ validation_pavilion.install_dir }}"
    creates: "{{ validation_pavilion.install_dir }}/venv/bin/activate"

- name: Upgrade pip tooling inside Pavilion virtualenv
  ansible.builtin.pip: # todo: figure out how to import dyanamic list so I dont need to hard code
    name:
      - pip==24.0
      - setuptools==69.0.3
      - wheel==0.42.0
    state: present # todo: 'latest' throws an error so this is current solution
    virtualenv: "{{ validation_pavilion.install_dir }}/venv"
    virtualenv_command: python3 -m venv

- name: Install Pavilion Python dependencies
  ansible.builtin.pip:
    requirements: requirements.txt
    virtualenv: "{{ validation_pavilion.install_dir }}/venv"
    virtualenv_command: python3 -m venv
  args:
    chdir: "{{ validation_pavilion.install_dir }}"


===== ./roles/validation/tasks/pavilion_config.yml =====
---
- name: Create Pavilion config directory
  ansible.builtin.file:
    path: "{{ validation_pavilion.config_dir }}"
    state: directory
    mode: "0775"
    recurse: yes

- name: Deploy pavilion.yaml (top-level Pavilion config)
  ansible.builtin.template:
    src: pavilion.yaml.j2
    dest: "{{ validation_pavilion.config_dir }}/pavilion.yaml"
    mode: "0644"

- name: Copy Pavilion suites ... # TODO: need to implement
  ansible.builtin.copy:
    dest: "{{ validation_pavilion.config_dir }}/{{ item.dest }}"
    content: "{{ lookup('template', role_path ~ '/' ~ item.src) }}"
    mode: "0644"
  loop:
    - { src: "pavilion/suites/controller_smoke.yml",
        dest: "suites/controller_smoke.yml" }
    - { src: "pavilion/suites/basic_smoke.yml",
        dest: "suites/basic_smoke.yml" }
    - { src: "pavilion/hosts/cluster.yml",
        dest: "hosts/cluster.yml" }


===== ./roles/pxe/handlers/main.yml =====
---
- name: Restart dnsmasq service
  ansible.builtin.service:
    name: dnsmasq
    state: restarted
  listen: restart dnsmasq

- name: Restart httpd service
  ansible.builtin.service:
    name: httpd
    state: restarted
  listen: restart httpd

- name: Reload firewalld service
  ansible.builtin.service:
    name: firewalld
    state: reloaded
  listen: reload firewalld


===== ./roles/pxe/templates/ks.cfg.j2 =====
#version=RHEL9
text
lang en_US.UTF-8
keyboard us
timezone UTC --utc

rootpw --iscrypted {{ pxe.root_password_hash }}
user --name={{ pxe.username }} --groups=wheel --iscrypted --password={{ pxe.local_user_password_hash | default(pxe.root_password_hash) }}
reboot

url --url "http://{{ pxe.server_ip }}/os"
network --bootproto=dhcp

firewall --enabled --service=ssh
selinux --enforcing

bootloader --location=mbr --boot-drive={{ pxe.install_drive | default('nvme0n1') }}
clearpart --all --initlabel
autopart --type=lvm

repo --name="slurm-local" --baseurl=http://{{ pxe.server_ip }}/slurm

%packages
@^minimal-environment
%end

%post
# Enable passwordless sudo for the PXE-created user
echo "{{ pxe.username | default('ansible') }} ALL=(ALL) NOPASSWD:ALL" > /etc/sudoers.d/99-{{ pxe.username | default('ansible') }}-nopasswd
chmod 0440 /etc/sudoers.d/99-{{ pxe.username | default('ansible') }}-nopasswd

echo "Installation complete" > /root/post.log
%end


===== ./roles/pxe/templates/dnsmasq.conf.j2 =====
# Do not place PXE or DHCP options here

user=dnsmasq
group=dnsmasq
bind-interfaces
domain-needed
bogus-priv
log-queries
log-dhcp

# Directory for additional dnsmasq configurations (PXE, DHCP, etc.)
conf-dir=/etc/dnsmasq.d,.rpmnew,.rpmsave,.rpmorig


===== ./roles/pxe/templates/boot.ipxe.j2 =====
#!ipxe
# 1. Let the user see what’s happening
echo Booting cluster image in 5 s ...
echo MAC: ${mac:hexhyp}  IP: ${ip}
echo Press ^C to abort, or 's' for shell ...

# 2. Countdown (5000 ms) that accepts keypress
prompt --timeout 5000 --key s shell && shell || goto boot_cluster

# 3. Select default target (fresh install vs. local disk)
:boot_cluster
set default_target {{ pxe_default_target | default('fresh') | lower }}
iseq ${default_target} local && goto boot_local ||
iseq ${default_target} fresh && goto boot_fresh || goto boot_fresh

:boot_fresh
# 4. HTTP installer with explicit stage2/repo/ks
kernel http://{{ pxe.server_ip }}/os/images/pxeboot/vmlinuz initrd=initrd.img inst.repo=http://{{ pxe.server_ip }}/os inst.ks=http://{{ pxe.server_ip }}/ks.cfg ip=dhcp rd.neednet=1 inst.text
initrd http://{{ pxe.server_ip }}/os/images/pxeboot/initrd.img
boot

:boot_local
# 5. Safety net: boot from NVMe
sanboot --no-describe --drive 0x80


===== ./roles/pxe/templates/pxe.conf.j2 =====
# Cluster PXE and DHCP configuration

port=0
listen-address={{ pxe.server_ip }}
# Bind dynamically so interface flaps don't kill the daemon; scope to our IP
bind-dynamic
except-interface=lo
log-dhcp
dhcp-authoritative

# DHCP range and options (from config/net.yml)
dhcp-range={{ pxe.dhcp_range_start }},{{ pxe.dhcp_range_end }},{{ pxe.dhcp_netmask }},12h
dhcp-option=3,{{ pxe.gateway }}
dhcp-option=6,{{ pxe.dns }}

# Static host reservations (from inventory)
{% for group in ['compute'] %}
{% for host in groups.get(group, []) %}
{% set host_ip = hostvars[host].ansible_host | default(host) %}
{% set macs = hostvars[host].get('nic_mac', []) %}
{% for mac in macs %}
{% if mac and mac != '00:00:00:00:00:00' %}
dhcp-host={{ mac | lower }},{{ host }},{{ host_ip }},infinite
{% endif %}
{% endfor %}
{% endfor %}
{% endfor %}

# iPXE detection and architecture tagging
dhcp-match=set:ipxe,option:user-class,"iPXE"
dhcp-match=set:efi64,option:client-arch,7
dhcp-match=set:efi64,option:client-arch,9
dhcp-match=set:efi64,option:client-arch,11
dhcp-match=set:efi64,option:client-arch,12

# Boot logic
dhcp-boot=tag:ipxe,http://{{ pxe.server_ip }}/boot.ipxe
dhcp-boot=tag:efi64,tag:!ipxe,{{ pxe.uefi_name }}
dhcp-boot=tag:!efi64,tag:!ipxe,undionly.kpxe

# Enable TFTP service
enable-tftp
tftp-root={{ pxe.tftp_root | default('/srv/tftp') }}

# PXE menu text and timeout
pxe-service=BC_EFI, "Network Boot (UEFI)", {{ pxe.uefi_name }}
pxe-prompt="Booting cluster installation...",{{ pxe.timeout_ms }}


===== ./roles/pxe/templates/apache_pxe.conf.j2 =====
# Hardened template for apache
Alias /os "{{ pxe.http_root }}/os"

<Directory "{{ pxe.http_root }}/os">
    Options +Indexes
    AllowOverride None
    Require all granted
</Directory>


===== ./roles/pxe/defaults/main.yml =====
---
# These are derived from the top-level inventory vars:
pxe:
  # General
  vendor: "{{ pxe_config.vendor | default('rocky') }}"
  image: "{{ pxe_config.image | default('rocky9_minimal') }}"

  # Paths
  http_root: "{{ pxe_config.paths.http_root }}"
  tftp_root: "{{ pxe_config.paths.tftp_root }}"
  iso_dir: "{{ pxe_config.paths.iso_dir }}"
  iso_mount: "{{ pxe_config.paths.iso_mount }}"
  iso_path: "{{ pxe_config.paths.iso_dir }}/{{ images[pxe_config.image | default('rocky9_minimal')].name | default(pxe_config.image ~ '.iso') }}"
  iso_url: "{{ images[pxe.image | default('rocky9_minimal')].url | default('') }}"

  # Networking
  boot_interface: "{{ network_config.pxe_iface }}"
  dhcp_subnet: "{{ network_config.subnet }}"
  dhcp_netmask: "{{ network_config.netmask }}"
  dhcp_range_start: "{{ network_config.dhcp_range.start }}"
  dhcp_range_end: "{{ network_config.dhcp_range.end }}"
  server_ip: "{{ network_config.server_ip }}"
  gateway: "{{ network_config.gateway | default(network_config.server_ip, true) }}"
  dns: "{{ network_config.dns | default(network_config.server_ip, true) }}"

  # Install options
  install_drive: "{{ pxe_config.install.drive | default('nvme0n1') }}"
  timeout_ms: "{{ pxe_config.ipxe.timeout_ms | default(5000) }}"
  default_target: "{{ pxe_config.ipxe.default_target | default('local') }}"
  menu_items: "{{ pxe_config.ipxe.menu | default([]) }}"

  # Binaries
  uefi_src: /usr/share/ipxe/ipxe-x86_64.efi
  uefi_name: ipxe.efi

  # Login details
  username: "{{ pxe_runtime_credentials.username | default('ansible') }}"
  # User must make hashes
  root_password_hash: "{{ pxe_runtime_credentials.root_password_hash }}"
  local_user_password_hash: "{{ pxe_runtime_credentials.local_user_password_hash }}"

# for firewall
firewall_allowed_subnets:
  - "10.0.0.0/24" # Cluster LAN
  - "192.168.68.0/24" # Home LAN


===== ./roles/pxe/meta/main.yml =====
---
galaxy_info:
  author: Gavin
  description: PXE provisioning for Rocky Linux cluster nodes
  license: MIT
  min_ansible_version: "2.15"
  platforms:
    - name: EL
      versions:
        - "9"

dependencies: []


===== ./roles/pxe/tasks/deploy_configs.yml =====
---
- name: Remove legacy dnsmasq PXE config (interface/bind leftovers)
  ansible.builtin.file:
    path: /etc/dnsmasq.d/pxe.conf
    state: absent
  tags:
    - pxe
    - pxe_configs

- name: Deploy PXE config
  ansible.builtin.template:
    src: pxe.conf.j2
    dest: /etc/dnsmasq.d/pxe.conf
    owner: root
    group: root
    mode: '0644'
    force: true
  notify: restart dnsmasq
  tags:
    - pxe
    - pxe_configs

- name: Deploy iPXE boot script to TFTP root
  ansible.builtin.template:
    src: boot.ipxe.j2
    dest: "{{ pxe.tftp_root }}/boot.ipxe"
    owner: root
    group: root
    mode: '0644'
  notify: restart dnsmasq
  tags:
    - pxe
    - pxe_configs

- name: Deploy iPXE boot script to HTTP root (for http:// chainload)
  ansible.builtin.template:
    src: boot.ipxe.j2
    dest: "{{ pxe.http_root }}/boot.ipxe"
    owner: root
    group: root
    mode: '0644'
  notify: restart httpd
  tags:
    - pxe
    - pxe_configs

- name: Deploy Kickstart configuration
  ansible.builtin.template:
    src: ks.cfg.j2
    dest: "{{ pxe.http_root }}/ks.cfg"
    owner: root
    group: root
    mode: '0644'
  notify: restart httpd
  tags:
    - pxe
    - pxe_configs

- name: Ensure /etc/sysconfig/dnsmasq has no interface overrides
  ansible.builtin.copy:
    dest: /etc/sysconfig/dnsmasq
    owner: root
    group: root
    mode: '0644'
    content: |
      # Managed by Ansible PXE role
      DNSMASQ_OPTS=""
  notify: restart dnsmasq
  tags:
    - pxe
    - pxe_configs

- name: Validate dnsmasq configuration syntax
  ansible.builtin.command: dnsmasq --test --conf-file=/etc/dnsmasq.conf --conf-dir=/etc/dnsmasq.d
  register: dnsmasq_test
  changed_when: false
  failed_when: dnsmasq_test.rc != 0
  tags:
    - pxe
    - pxe_configs
    - pxe_services


===== ./roles/pxe/tasks/main.yml =====
---
- name: PXE
  block:
    - import_tasks: preflight.yml
      tags:
        - pxe
        - pxe_preflight
    - import_tasks: firewall.yml
      tags:
        - pxe
        - pxe_firewall
    - import_tasks: httpd.yml
      tags:
        - pxe
        - httpd
    - import_tasks: iso.yml
      tags:
        - pxe
        - pxe_iso
    - import_tasks: binaries.yml
      tags:
        - pxe
        - pxe_binaries
    - import_tasks: deploy_configs.yml
      tags:
        - pxe
        - pxe_configs
    - import_tasks: services.yml
      tags:
        - pxe
        - pxe_services
    - import_tasks: validate.yml
      tags:
        - pxe
        - pxe_validate


===== ./roles/pxe/tasks/iso.yml =====
---
- name: Ensure ISO present
  ansible.builtin.get_url:
    url: "{{ pxe.iso_url }}"
    dest: "{{ pxe.iso_path }}"
    mode: "0644"

- name: Mount ISO
  ansible.posix.mount:
    path: "{{ pxe.iso_mount }}"
    src: "{{ pxe.iso_path }}"
    fstype: iso9660
    opts: ro,loop
    state: mounted


===== ./roles/pxe/tasks/validate.yml =====
---
- name: Validate PXE environment readiness
  block:

    - name: Assert PXE directories exist
      ansible.builtin.assert:
        that:
          - pxe.tftp_root is defined
          - pxe.http_root is defined
          - pxe.iso_mount is defined
        fail_msg: "PXE directory variables are undefined."
        success_msg: "PXE directory paths are defined."

    - name: Check that PXE directories exist on disk
      ansible.builtin.stat:
        path: "{{ item }}"
      loop:
        - "{{ pxe.tftp_root }}"
        - "{{ pxe.http_root }}"
        - "{{ pxe.iso_mount }}"
      register: pxe_dirs

    - name: Validate PXE directories presence
      ansible.builtin.assert:
        that:
          - pxe_dirs.results | map(attribute='stat.exists') | min
        fail_msg: "One or more PXE directories missing (TFTP/HTTP/ISO mount)."
        success_msg: "All PXE directories present."

    - name: Verify iPXE binary staged correctly
      ansible.builtin.stat:
        path: "{{ pxe.tftp_root }}/{{ pxe.uefi_name }}"
      register: ipxe_bin

    - name: Assert iPXE binary presence
      ansible.builtin.assert:
        that:
          - ipxe_bin.stat.exists
          - ipxe_bin.stat.size > 100000 # real ipxe.efi is 750-800KB
        fail_msg: "Missing or empty iPXE binary in {{ pxe.tftp_root }}."
        success_msg: "iPXE binary present."

    - name: Verify ISO mount contents
      ansible.builtin.stat:
        path: "{{ pxe.iso_mount }}/images/pxeboot/vmlinuz"
      register: iso_kernel

    - name: Assert ISO is properly mounted
      ansible.builtin.assert:
        that:
          - iso_kernel.stat.exists
        fail_msg: "ISO not mounted or invalid. Check {{ pxe.iso_mount }}."
        success_msg: "ISO mount valid and kernel accessible."

    - name: Validate dnsmasq PXE config presence
      ansible.builtin.stat:
        path: /etc/dnsmasq.d/pxe.conf
      register: dnsmasq_cfg

    - name: Assert PXE config deployed
      ansible.builtin.assert:
        that:
          - dnsmasq_cfg.stat.exists
          - dnsmasq_cfg.stat.size > 200
        fail_msg: "Missing or incomplete PXE configuration in /etc/dnsmasq.d/pxe.conf."
        success_msg: "PXE configuration file exists."

    - name: Check PXE services running
      ansible.builtin.service_facts:

    - name: Assert core PXE services active
      ansible.builtin.assert:
        that:
          - ansible_facts.services.get('dnsmasq.service', {}).get('state') == 'running'
          - ansible_facts.services.get('httpd.service', {}).get('state') == 'running'
          - ansible_facts.services.get('firewalld.service', {}).get('state') == 'running'
        fail_msg: "PXE core services are not active. Verify dnsmasq, httpd, and firewalld."
        success_msg: "PXE services running as expected."


===== ./roles/pxe/tasks/binaries.yml =====
---
- name: Stage iPXE binaries
  ansible.builtin.copy:
    src: "{{ item.src }}"
    dest: "{{ pxe.tftp_root }}/{{ item.dest }}"
    remote_src: true
    owner: root
    group: root
    mode: '0644'
  loop:
    - src: "{{ pxe.uefi_src }}"
      dest: "{{ pxe.uefi_name }}"
  when: not ansible_check_mode


===== ./roles/pxe/tasks/firewall.yml =====
---
# 0. Ensure firewalld is installed and running
# handling here instead of preflight.yml to isolate firewall logic
- name: Ensure firewalld is installed
  ansible.builtin.package:
    name: firewalld
    state: present

- name: Ensure firewalld is enabled and running
  ansible.builtin.service:
    name: firewalld
    state: started
    enabled: yes

- name: Determine firewall guard requirements
  ansible.builtin.set_fact:
    firewall_target_host: "{{ ansible_host | default(inventory_hostname) }}"
    firewall_needs_ssh_guard: "{{ ((ansible_connection | default('ssh')) != 'local') | bool }}"

# 1. Detect SSH client IP and create a temporary allow rule
- name: Determine SSH source for guard
  ansible.builtin.set_fact:
    ssh_source_ip: >-
      {{ (ansible_env.SSH_CLIENT.split()[0]
          if (ansible_env.SSH_CLIENT | default('')) | length > 0
          else ansible_host | default(inventory_hostname)) | default('') }}
  when: firewall_needs_ssh_guard | bool

- name: Add temporary ACCEPT for current SSH source (runtime only)
  ansible.posix.firewalld:
    source: "{{ ssh_source_ip }}/32"
    state: enabled
    zone: public
    immediate: yes
    permanent: no
  when:
    - firewall_needs_ssh_guard | bool
    - ssh_source_ip | default('') | length > 0

- name: Validate SSH connectivity before applying firewall changes (pre-change)
  ansible.builtin.wait_for:
    host: "{{ firewall_target_host }}"
    port: 22
    timeout: 2
  when:
    - firewall_needs_ssh_guard | bool
    - ssh_source_ip | default('') | length > 0

# 2. Baseline firewall configuration
- name: Allow SSH service permanently
  ansible.posix.firewalld:
    service: ssh
    state: enabled
    zone: public
    permanent: yes

# Allow inbound subnets defined in defaults
# (see firewall_allowed_subnets in defaults/main.yml)
- name: Allow cluster internal networks
  ansible.posix.firewalld:
    source: "{{ item }}"
    state: enabled
    zone: public
    permanent: yes
  loop: "{{ firewall_allowed_subnets }}"

# 3. HPC-relevant permitted services
# PXE/dnsmasq/httpd allowed only on the controller
- name: Allow PXE + provisioning services on controller
  ansible.posix.firewalld:
    service: "{{ item }}"
    state: enabled
    zone: public
    permanent: yes
  loop:
    - http
    - tftp
    - dhcp
  when: inventory_hostname in groups['controller']

- name: Allow Slurm scheduler ports on controller
  ansible.posix.firewalld:
    port: "{{ item }}"
    state: enabled
    zone: public
    permanent: yes
  loop:
    - "6817/tcp"
    - "6818/tcp"
  when: inventory_hostname in groups['controller']

# 4. Commit firewall changes safely
- name: Reload firewalld to commit permanent changes
  ansible.builtin.service:
    name: firewalld
    state: reloaded
  changed_when: false

# 5. Validate SSH connectivity *after* applying new rules
- name: Validate SSH connectivity (post-change)
  ansible.builtin.wait_for:
    host: "{{ firewall_target_host }}"
    port: 22
    timeout: 2
  when:
    - firewall_needs_ssh_guard | bool
    - ssh_source_ip | default('') | length > 0

# 6. Remove temporary SSH allow (runtime only)
- name: Remove temporary SSH allow rule
  ansible.posix.firewalld:
    source: "{{ ssh_source_ip }}/32"
    state: disabled
    zone: public
    immediate: yes
    permanent: no
  when:
    - firewall_needs_ssh_guard | bool
    - ssh_source_ip | default('') | length > 0


===== ./roles/pxe/tasks/httpd.yml =====
---
- name: Install webserver for PXE
  package:
    name: httpd
    state: present

- name: Deploy PXE Apache config
  template:
    src: apache_pxe.conf.j2
    dest: /etc/httpd/conf.d/pxe.conf
    owner: root
    group: root
    mode: '0644'
  notify:
    - restart httpd


===== ./roles/pxe/tasks/services.yml =====
---
- name: Ensure PXE services enabled and running
  ansible.builtin.service:
    name: "{{ item }}"
    enabled: true
    state: started
  loop:
    - httpd
    - dnsmasq
  tags:
    - pxe
    - pxe_services


===== ./roles/pxe/tasks/preflight.yml =====
---
- name: Install PXE server packages
  package:
    name:
      - dnsmasq
      - tftp-server
      - ipxe-bootimgs
      - httpd
      - python3-libselinux
      - policycoreutils-python-utils
    state: present

- name: Ensure PXE credential hashes are provided
  ansible.builtin.assert:
    that:
      - pxe.root_password_hash is defined
      - pxe.root_password_hash | length > 0
      - pxe.local_user_password_hash is defined
      - pxe.local_user_password_hash | length > 0
    fail_msg: >-
      PXE credential hashes missing.
      Populate PXE_ROOT_PASSWORD_HASH and PXE_LOCAL_USER_PASSWORD_HASH.

- name: Ensure TFTP directory exists
  file:
    path: "{{ pxe.tftp_root }}"
    state: directory
    owner: root
    group: root
    mode: "0755"

- name: Label TFTP directory for SELinux
  community.general.sefcontext:
    target: "{{ pxe.tftp_root }}(/.*)?"
    setype: tftpdir_t
    state: present
  when:
    - ansible_selinux is defined
    - ansible_selinux.status == "enabled"

- name: Restore SELinux context on TFTP directory
  ansible.builtin.command: restorecon -Fr {{ pxe.tftp_root }}
  changed_when: false
  when:
    - ansible_selinux is defined
    - ansible_selinux.status == "enabled"

- name: Ensure ISO directory exists
  ansible.builtin.file:
    path: "{{ pxe.iso_dir }}"
    state: directory
    mode: "0755"

- name: Ensure ISO mount directory exists
  ansible.builtin.file:
    path: "{{ pxe.iso_mount }}"
    state: directory
    mode: "0755"

- name: Check PXE interface exists
  ansible.builtin.stat:
    path: "/sys/class/net/{{ pxe.boot_interface }}"
  register: pxe_iface_stat
  when:
    - pxe.boot_interface is defined
    - pxe.boot_interface | length > 0

- name: Assert PXE interface is present
  ansible.builtin.assert:
    that:
      - pxe_iface_stat.stat.exists | default(false)
    fail_msg: >-
      PXE interface {{ pxe.boot_interface }} missing. Update network_config.pxe_iface
      in config/net.yml to a valid interface name on this host.
  when:
    - pxe_iface_stat is defined

- name: Bring PXE interface up (best effort)
  ansible.builtin.command: ip link set dev {{ pxe.boot_interface }} up
  changed_when: false
  failed_when: false
  when:
    - pxe_iface_stat is defined
    - pxe_iface_stat.stat.exists | default(false)

- name: Assert PXE server IP is configured locally
  ansible.builtin.assert:
    that:
      - pxe.server_ip in ansible_all_ipv4_addresses
    fail_msg: >-
      PXE server_ip {{ pxe.server_ip }} is not assigned on this host. Ensure
      network_config.server_ip in config/net.yml matches an address on this
      machine (and that the interface is up) before starting dnsmasq.

- name: Deploy neutral /etc/dnsmasq.conf
  ansible.builtin.copy:
    dest: /etc/dnsmasq.conf
    owner: root
    group: root
    mode: '0644'
    content: |
      # Managed by Ansible PXE role
      user=dnsmasq
      group=dnsmasq
      conf-dir=/etc/dnsmasq.d,.rpmnew,.rpmsave,.rpmorig
  notify: restart dnsmasq

- name: Ensure Apache root directory exists
  ansible.builtin.file:
    path: /var/www/html
    state: directory
    owner: root
    group: root
    mode: "0755"


===== ./roles/controller/handlers/main.yml =====
---
- name: Restart sshd
  ansible.builtin.service:
    name: sshd
    state: restarted

- name: Reload NetworkManager
  ansible.builtin.service:
    name: NetworkManager
    state: reloaded


===== ./roles/controller/templates/ifcfg.j2 =====
[connection]
id={{ network_config.pxe_iface }}
type=ethernet
interface-name={{ network_config.pxe_iface }}
autoconnect=true

[ipv4]
method=manual
address1={{ network_config.server_ip }}/{{ network_config.subnet | default('10.0.0.0/24') | ansible.utils.ipaddr('prefix') }},{{ network_config.gateway | default(network_config.server_ip, true) }}
dns={{ network_config.dns }}
may-fail=false


===== ./roles/controller/defaults/main.yml =====
---
# These are derived from the top-level inventory vars:

# Repository Paths
cluster_repo_root: "{{ cluster_repo_root }}"
cluster_config_root: "{{ cluster_config_root }}"
cluster_meta_root: "{{ cluster_meta_root }}"

# Core Cluster Metadata
cluster_name: "{{ cluster_name | default('cluster') }}"
cluster_description: "{{ cluster_description | default('High-performance cluster controller') }}"

# Python / Virtual Environment
python_version: "3.9"
python_executable: "/usr/bin/python3"
python_system_packages:
  - python3
  - python3-devel
venv_path: "{{ cluster_repo_root }}/.venv"
venv_bin: "{{ venv_path }}/bin"
venv_python: "{{ venv_bin }}/python"
venv_pip: "{{ venv_bin }}/pip"
venv_ansible: "{{ venv_bin }}/ansible"

venv_bootstrap_packages:
  - pip==24.0
  - setuptools==69.0.3
  - wheel==0.42.0

controller_requirements_file: "{{ cluster_repo_root }}/requirements/controller.txt"
ansible_collections_requirements: "{{ cluster_repo_root }}/requirements/requirements.yml"
ansible_core_expected_version: "2.15.8"  # Keep in sync with requirements/controller.txt

# Controller Identity
controller_user: "{{ ansible_user | default('ansible') }}"
controller_group: "{{ controller_user }}"
controller_mode: "0755"
controller_hostname: "{{ inventory_hostname }}"

# Networking (from network_config in inventory)
controller_interface: "{{ network_config.pxe_iface }}"
controller_ip: "{{ network_config.server_ip }}"
controller_gateway: "{{ network_config.gateway | default(network_config.server_ip, true) }}"
controller_dns: "{{ network_config.dns }}"
controller_subnet: "{{ network_config.subnet }}"
controller_dhcp_range_start: "{{ network_config.dhcp_range.start }}"
controller_dhcp_range_end: "{{ network_config.dhcp_range.end }}"

# PXE Integration
pxe_install_drive: "{{ pxe_config.install.drive | default('nvme0n1') }}"
pxe_default_target: "{{ pxe_config.ipxe.default_target | default('local') }}"


===== ./roles/controller/meta/main.yml =====
---
galaxy_info:
  author: Gavin
  description: Controller role for Rocky cluster — bootstraps head node, venv, and network identity
  license: MIT
  min_ansible_version: "2.15"
  platforms:
    - name: EL
      versions:
        - "9"

dependencies: []


===== ./roles/controller/tasks/hostname.yml =====
---
- name: Set controller hostname
  ansible.builtin.hostname:
    name: "{{ controller_hostname | default(inventory_hostname) }}"
  tags:
    - network
    - hostname


===== ./roles/controller/tasks/validate_ip.yml =====
---
- name: Confirm controller IP matches network_config.server_ip
  ansible.builtin.assert:
    that:
      - network_config.server_ip in ansible_all_ipv4_addresses
    fail_msg: "Controller IP mismatch."


===== ./roles/controller/tasks/main.yml =====
---
- name: Controller
  block:
    # - import_tasks: venv.yml ; using dev venv for now for testing
    #   tags:
    #     - venv
    #     - controller
    - import_tasks: hostname.yml
      tags:
        - hostname
        - controller
    - import_tasks: static_ip.yml
      tags:
        - static_ip
        - controller
    - import_tasks: validate_ip.yml
      tags:
        - validate
        - controller


===== ./roles/controller/tasks/static_ip.yml =====
---
- name: Configure static IP for controller
  ansible.builtin.template:
    src: ifcfg.j2
    dest: "/etc/NetworkManager/system-connections/{{ network_config.pxe_iface }}.nmconnection"
    owner: root
    group: root
    mode: '0600'

- name: Reload NetworkManager to apply config
  ansible.builtin.systemd:
    name: NetworkManager
    state: restarted


===== ./roles/controller/tasks/venv.yml =====
---
- name: Ensure Python runtime packages present
  ansible.builtin.package:
    name: "{{ python_system_packages }}"
    state: present
  tags: venv

- name: Verify Python 3 binary exists
  ansible.builtin.stat:
    path: "{{ python_executable }}"
  register: python_bin
  failed_when: not python_bin.stat.exists
  tags: venv

- name: Validate Python version matches {{ python_version }}
  ansible.builtin.command:
    cmd: "{{ python_executable }} --version"
  register: python_version_check
  changed_when: false
  failed_when: "'Python {{ python_version }}' not in ((python_version_check.stdout | default('')) ~ (python_version_check.stderr | default('')))"
  tags: venv

- name: Ensure virtual environment directory exists
  ansible.builtin.file:
    path: "{{ venv_path }}"
    state: directory
    mode: '0755'
  tags: venv

- name: Create Python virtual environment (idempotent)
  ansible.builtin.command:
    cmd: "{{ python_executable }} -m venv {{ venv_path }}"
  args:
    creates: "{{ venv_path }}/bin/activate"
  tags: venv

- name: Upgrade pip and core tooling inside venv
  ansible.builtin.pip:
    name: "{{ venv_bootstrap_packages }}"
    state: present
    virtualenv: "{{ venv_path }}"
    virtualenv_command: "{{ python_executable }} -m venv"
  tags: venv

- name: Install controller dependencies inside venv
  ansible.builtin.pip:
    requirements: "{{ controller_requirements_file }}"
    virtualenv: "{{ venv_path }}"
    virtualenv_command: "{{ python_executable }} -m venv"
  tags: venv

- name: Install required Ansible collections
  community.general.ansible_galaxy_install:
    type: collection
    requirements_file: "{{ ansible_collections_requirements }}"
    state: latest
  environment:
    PATH: "{{ venv_bin }}:{{ ansible_env.PATH | default('/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin') }}"
  tags: venv

- name: Verify Ansible installation inside venv
  ansible.builtin.command:
    cmd: "{{ venv_ansible }} --version"
  register: ansible_check
  changed_when: false
  failed_when: "'[core {{ ansible_core_expected_version }}]' not in ansible_check.stdout"
  tags: venv

- name: Print venv activation hint
  ansible.builtin.debug:
    msg: |
      Virtualenv ready:
        source {{ venv_path }}/bin/activate
  tags: venv


===== ./.env.example =====
PXE_ROOT_PASSWORD_HASH="$6$rounds ..."
PXE_LOCAL_USER_PASSWORD_HASH="$6$rounds ..."

